# Multimodal Transformer Modelling of Upper-Face Gestures From Text and Speech


**Abstract**: We  propose  a  semantically-aware  speech  driven  method  to generate expressive and natural upper-facial and head motion for Embodied Conversational Agents (ECA). In this work, we tackle two key challenges: produce natural and continuous (i) head  motion,  and  (ii)  upper-facial  gestures.   We  propose  a model that generates gestures based on multimodal input features:  the first modality is text, and the second one is speech prosody.   Our  model  makes  use  of  Transformers  and  Con-volutions to map the multimodal features that correspond to an utterance to continuous eyebrows and head gestures. 


The following videos show our model's predictions simulated on the Virtual Agent GRETA:

https://youtu.be/AWwP4X7CJvQ

https://youtu.be/i0YKoButSgw

https://www.youtube.com/watch?v=y8rwcuaI7s0

https://www.youtube.com/watch?v=9p2ADHjY0r0

https://youtu.be/eat08tVPPKM

https://youtu.be/lOsuWUTkkuI

https://youtu.be/EFg_t9PzXsA

https://youtu.be/pNHG4m4JheM


We will publish our code soon.
