We  propose  a  semantically-aware  speech  driven  method  togenerate expressive and natural upper-facial and head motion for Embodied Conversational Agents (ECA). In this work, we tackle two key challenges: produce natural and continuous (i)head  motion,  and  (ii)  upper-facial  gestures.   We  propose  a model that generates gestures based on multimodal input fea-tures:  the first modality is text, and the second one is speechprosody.   Our  model  makes  use  of  Transformers  and  Con-volutions to map the multimodal features that correspond to an utterance to continuous eyebrows and head gestures. 
