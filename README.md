We  propose  a  semantically-aware  speech  driven  method  togenerate expressive and natural upper-facial and head motion for Embodied Conversational Agents (ECA). In this work, we tackle two key challenges: produce natural and continuous (i)head  motion,  and  (ii)  upper-facial  gestures.   We  propose  a model that generates gestures based on multimodal input fea-tures:  the first modality is text, and the second one is speechprosody.   Our  model  makes  use  of  Transformers  and  Con-volutions to map the multimodal features that correspond to an utterance to continuous eyebrows and head gestures. 


https://youtu.be/AWwP4X7CJvQ

https://youtu.be/i0YKoButSgw

https://www.youtube.com/watch?v=y8rwcuaI7s0

https://www.youtube.com/watch?v=9p2ADHjY0r0

https://youtu.be/eat08tVPPKM

https://youtu.be/lOsuWUTkkuI

https://youtu.be/EFg_t9PzXsA

https://youtu.be/pNHG4m4JheM

